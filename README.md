# GGUF
GGUF Формат/Описание/Заметки #gguf #.gguf 

GGUF (GGML Universal Format) - это формат файла для хранения моделей для вывода с использованием GGML и исполнителей на базе GGML. GGUF является бинарным форматом, разработанным для быстрой загрузки и сохранения моделей, а также для удобства чтения. Модели традиционно разрабатываются с использованием PyTorch или другого фреймворка, а затем преобразуются в GGUF для использования в GGML.

GGUF является преемником форматов файлов GGML, GGMF и GGJT и разработан для однозначности путем включения всей необходимой информации для загрузки модели. Он также разработан с учетом расширяемости, чтобы можно было добавлять новую информацию к моделям без нарушения совместимости.

Для получения дополнительной информации о мотивации, стоящей за GGUF, см. раздел "Историческое состояние дел".

Спецификация
------------

GGUF является форматом на основе существующего GGJT, но вносит несколько изменений в формат для повышения расширяемости и удобства использования. Желательны следующие возможности:

* Развертывание в один файл: они легко распространяются и загружаются и не требуют каких-либо внешних файлов для дополнительной информации.
* Расширяемость: в исполнители GGML могут быть добавлены новые функции/в модели GGUF могут быть добавлены новые данные без нарушения совместимости с существующими моделями.
* Совместимость с mmap: модели могут быть загружены с использованием mmap для быстрой загрузки и сохранения.
* Легкость использования: модели могут быть легко загружены и сохранены с помощью небольшого количества кода, без необходимости внешних библиотек, независимо от используемого языка.
* Полная информация: все необходимая информация для загрузки модели содержится в файле модели, и пользователю не требуется предоставлять дополнительную информацию.

Ключевое отличие между GGJT и GGUF заключается в использовании структуры "ключ-значение" для гиперпараметров (теперь называемых метаданными), а не списка нетипизированных значений. Это позволяет добавлять новые метаданные без нарушения совместимости с существующими моделями и аннотировать модель дополнительной информацией, которая может быть полезной для вывода или для идентификации модели.

Структура файла
--------------

![image](https://github.com/ggerganov/ggml/assets/1991296/c3623641-3a1d-408e-bfaf-1b7c4e16aa63)
*диаграмма от [@mishig25](https://github.com/mishig25) (GGUF v3)*

Файлы GGUF устроены следующим образом. Они используют глобальное выравнивание, указанное в метаданных `general.alignment`, на которое ссылаются ниже как `ALIGNMENT`. При необходимости файл дополняется нулевыми байтами `0x00` до следующего кратного `general.alignment`.

Поля, включая массивы, записываются последовательно без выравнивания, если не указано иное.

Модели по умолчанию являются little-endian. Они также могут быть big-endian для использования с big-endian компьютерами; в этом случае все значения (включая значения метаданных и тензоров) также будут big-endian. На момент написания нет способа определить, является ли модель big-endian; это может быть исправлено в будущих версиях. Если не предоставляется дополнительная информация, считайте, что модель little-endian.

```c
enum ggml_type: uint32_t {
    GGML_TYPE_F32  = 0,
    GGML_TYPE_F16  = 1,
    GGML_TYPE_Q4_0 = 2,
    GGML_TYPE_Q4_1 = 3,
    // GGML_TYPE_Q4_2 = 4, поддержка удалена
    // GGML_TYPE_Q4_3 (5) поддержка удалена
    GGML_TYPE_Q5_0 = 6,
    GGML_TYPE_Q5_1 = 7,
    GGML_TYPE_Q8_0 = 8,
    GGML_TYPE_Q8_1 = 9,
    // k-квантизации
    GGML_TYPE_Q2_K = 10,
    GGML_TYPE_Q3_K = 11,
    GGML_TYPE_Q4_K = 12,
    GGML_TYPE_Q5_K = 13,
    GGML_TYPE_Q6_K = 14,
    GGML_TYPE_Q8_K = 15,
    GGML_TYPE_I8,
    GGML_TYPE_I16,
    GGML_TYPE_I32,
    GGML_TYPE_COUNT,
};

enum gguf_metadata_value_type: uint32_t {
    // Значение является 8-разрядным беззнаковым целым числом.
    GGUF_METADATA_VALUE_TYPE_UINT8 = 0,
    // Значение является 8-разрядным целым числом со знаком.
    GGUF_METADATA_VALUE_TYPE_INT8 = 1,
    // Значение является 16-разрядным беззнаковым целым числом little-endian.
    GGUF_METADATA_VALUE_TYPE_UINT16 = 2,
    // Значение является 16-разрядным целым числом со знаком little-endian.
    GGUF_METADATA_VALUE_TYPE_INT16 = 3,
    // Значение является 32-разрядным беззнаковым целым числом little-endian.
    GGUF_METADATA_VALUE_TYPE_UINT32 = 4,
    // Значение является 32-разрядным целым числом со знаком little-endian.
    GGUF_METADATA_VALUE_TYPE_INT32 = 5,
    // Значение является 32-разрядным числом с плавающей запятой IEEE754.
    GGUF_METADATA_VALUE_TYPE_FLOAT32 = 6,
    // Значение является логическим.
    // 1-байтовое значение, где 0 - ложь, а 1 - истина.
    // Все остальное недопустимо и должно рассматриваться либо как недействительная модель, либо как ошибка в чтении.
    GGUF_METADATA_VALUE_TYPE_BOOL = 7,
    // Значение является UTF-8 строкой, не завершающейся нуlem, с предваряющей длиной.
    GGUF_METADATA_VALUE_TYPE_STRING = 8,
    // Значение является массивом других значений, с предваряющей длиной и типом.
    ///
    // Массивы могут быть вложены, и длина массива - это количество элементов в массиве, а не количество байтов.
    GGUF_METADATA_VALUE_TYPE_ARRAY = 9,
    // Значение является 64-разрядным беззнаковым целым числом little-endian.
    GGUF_METADATA_VALUE_TYPE_UINT64 = 10,
    // Значение является 64-разрядным целым числом со знаком little-endian.
    GGUF_METADATA_VALUE_TYPE_INT64 = 11,
    // Значение является 64-разрядным числом с плавающей запятой IEEE754.
    GGUF_METADATA_VALUE_TYPE_FLOAT64 = 12,
}

// Строка в GGUF.
struct gguf_string_t {
    // Длина строки в байтах.
    uint64_t len;
    // Строка в виде UTF-8 строки без завершающего нуля.
    char string[len];
}

union gguf_metadata_value_t {
    uint8_t uint8;
    int8_t int8;
    uint16_t uint16;
    int16_t int16;
    uint32_t uint32;
    int32_t int32;
    float float32;
    uint64_t uint64;
    int64_t int64;
    double float64;
    bool bool_;
    gguf_string_t string;
    struct {
        // Любой тип значения допустим, включая массивы.
        gguf_metadata_value_type type;
        // Количество элементов, а не байтов
        uint64_t len;
        // Массив значений.
        gguf_metadata_value_t array[len];
    } array;
};

struct gguf_metadata_kv_t {
    // Ключ метаданных. Это стандартная строка GGUF с следующими оговорками:
    // - Он должен быть допустимой ASCII-строкой.
    // - Он должен быть иерархическим ключом, где каждый сегмент является `lower_snake_case` и разделен точкой `.`.
    // - Он должен быть не более 2^16-1/65535 байт.
    // Все ключи, не соответствующие этим правилам, недействительны.
    gguf_string_t key;

    // Тип значения.
    // Должен быть одним из значений `gguf_metadata_value_type`.
    gguf_metadata_value_type value_type;
    // Значение.
    gguf_metadata_value_t value;
};

struct gguf_header_t {
    // Магическое число для обозначения того, что это файл GGUF.
    // Должно быть `GGUF` на уровне байтов: `0x47` `0x47` `0x55` `0x46`.
    // Ваш исполнитель может использовать порядок байтов little-endian, поэтому это может быть
    // проверка на 0x46554747 и отмена порядка байтов.
    // Рассмотрите возможность быть очень явным в отношении порядка байтов здесь.
    uint32_t magic;
    // Версия реализуемого формата.
    // Должно быть `3` для версии, описанной в этом спецификации, которая вводит поддержку big-endian.
    //
    // Эта версия должна увеличиваться только при структурных изменениях формата.
    // Изменения, не влияющие на структуру файла, должны вместо этого обновлять метаданные
    // для обозначения изменения.
    uint32_t version;
    // Количество тензоров в файле.
    // Это явно, а не включено в метаданные, чтобы гарантировать, что оно всегда присутствует
    // при загрузке тензоров.
    uint64_t tensor_count;
    // Количество пар "ключ-значение" метаданных.
    uint64_t metadata_kv_count;
    // Пары "ключ-значение" метаданных.
    gguf_metadata_kv_t metadata_kv[metadata_kv_count];
};

uint64_t align_offset(uint64_t offset) {
    return offset + (ALIGNMENT - (offset % ALIGNMENT)) % ALIGNMENT;
}

struct gguf_tensor_info_t {
    // Имя тензора. Это стандартная строка GGUF с оговоркой, что
    // она должна быть не более 64 байт.
    gguf_string_t name;
    // Количество измерений в тензоре.
    // В настоящее время не более 4, но это может измениться в будущем.
    uint32_t n_dimensions;
    // Размерности тензора.
    uint64_t dimensions[n_dimensions];
    // Тип тензора.
    ggml_type type;
    // Смещение данных тензора в этом файле в байтах.
    //
    // Это смещение относительно `tensor_data`, а не относительно начала файла,
    // что облегчает запись файла для ��второв.
    // Читатели должны рассматривать возможность предоставления этого смещения относительно
    // файла, чтобы облегчить чтение данных.
    //
    // Должно быть кратным `ALIGNMENT`. То есть, `align_offset(offset) == offset`.
    uint64_t offset;
};

struct gguf_file_t {
    // Заголовок файла.
    gguf_header_t header;

    // Информация о тензорах, которая может быть использована для поиска данных тензора.
    gguf_tensor_info_t tensor_infos[header.tensor_count];

    // Выравнивание до ближайшего кратного `ALIGNMENT`.
    //
    // То есть, если `sizeof(header) + sizeof(tensor_infos)` не является кратным `ALIGNMENT`,
    // эта выравнивание добавляется для этого.
    //
    // Это можно вычислить как `align_offset(position) - position`, где `position` -
    // позиция конца `tensor_infos` (т.е. `sizeof(header) + sizeof(tensor_infos)`).
    uint8_t _padding[];

    // Данные тензора.
    //
    // Это произвольные двоичные данные, соответствующие весам модели. Эти данные должны быть близки
    // или идентичны данным в исходном файле модели, но могут отличаться из-за квантизации или
    // других оптимизаций для вывода. Любые такие отклонения должны быть зафиксированы в метаданных или в качестве
    // части определения архитектуры.
    //
    // Данные каждого тензора должны храниться в этом массиве и располагаться через его `tensor_infos` запись.
    // Смещение данных каждого тензора должно быть кратным `ALIGNMENT`, и пространство между тензорами
    // должно быть заполнено до `ALIGNMENT` байтов.
    uint8_t tensor_data[];
};
```

Стандартизированные пары "ключ-значение"
----------------------------------------

Следующие пары "ключ-значение" стандартизированы. Этот список может расширяться в будущем по мере открытия новых случаев использования. При необходимости имена совпадают с оригинальными определениями моделей, чтобы облегчить сопоставление между ними.

Не все из этих параметров обязательны, но они все рекомендуются. Ключи, которые обязательны, выделены жирным шрифтом. При отсутствии пар читатель должен считать, что значение неизвестно, и либо использовать значение по умолчанию, либо выдать ошибку в зависимости от ситуации.

Сообщество может разрабатывать свои собственные пары "ключ-значение" для передачи дополнительных данных. Однако они должны быть пространством имен с соответствующим именем сообщества, чтобы избежать коллизий. Например, сообщество `rustformers` может использовать `rustformers.` в качестве префикса для всех своих ключей.

Если определенный ключ сообщества широко используется, он может быть повышен до стандартизированного ключа.

По соглашению, большинство счетчиков/длин/и т.д. являются `uint64`, если не указано иное. Это позволяет поддерживать более крупные модели в будущем. Некоторые модели могут использовать `uint32` для своих значений; рекомендуется читателям поддерживать оба варианта.

### Общее

#### Обязательно

* **`general.architecture: string`**: описывает, какую архитектуру р��ализует эта модель. Только `[a-z0-9]+` символы ASCII в нижнем регистре. Известные значения включают:
  * `llama`
  * `mpt`
  * `gptneox`
  * `gptj`
  * `gpt2`
  * `bloom`
  * `falcon`
  * `mamba`
  * `rwkv`
* **`general.quantization\_version: uint32`**: Версия формата квантизации. Не требуется, если модель не квантизована (т.е. ни один тензор не квантизован). Если какие-либо тензоры квантизованы, это _должно_ присутствовать. Это отдельно от схемы квантизации тензоров самого; версия квантизации может меняться без изменения имени схемы (например, схема квантизации - Q5\_K, а версия квантизации - 4).
* **`general.alignment: uint32`**: глобальное выравнивание для использования, как описано выше. Это может варьироваться для разрешения различных схем выравнивания, но оно должно быть кратным 8. Некоторые авторы могут не записывать выравнивание. Если выравнивание **не** указано, считайте, что оно равно `32`.

#### Общие метаданные

* `general.name`: Имя модели. Это должно быть человекочитаемое имя, которое может быть использовано для идентификации модели. Он должен быть уникален в пределах сообщества, в котором определена модель.
* `general.author`: Автор модели.
* `general.url`: URL домашней страницы модели. Это может быть репозиторий GitHub, статья и т.д.
* `general.description: string`: свободная форма описания модели, включая все, что не покрывается другими полями
* `general.license: string`: Лицензия модели, выраженная как [SPDX выражение лицензии](https://spdx.github.io/spdx-spec/v2-draft/SPDX-license-expressions/) (например, `"MIT OR Apache-2.0"`). Не включайте никакой другой информации, такой как текст лицензии или URL лицензии.
* `general.file_type: uint32`: Перечисленное значение, описывающее тип большинства тензоров в файле. Необязательно; может быть выведено из типов тензоров.
  * `ALL_F32 = 0`
  * `MOSTLY_F16 = 1`
  * `MOSTLY_Q4_0 = 2`
  * `MOSTLY_Q4_1 = 3`
  * `MOSTLY_Q4_1_SOME_F16 = 4`
  * `MOSTLY_Q4_2 = 5` (поддержка удалена)
  * `MOSTLY_Q4_3 = 6` (поддержка удалена)
  * `MOSTLY_Q8_0 = 7`
  * `MOSTLY_Q5_0 = 8`
  * `MOSTLY_Q5_1 = 9`
  * `MOSTLY_Q2_K = 10`
  * `MOSTLY_Q3_K_S = 11`
  * `MOSTLY_Q3_K_M = 12`
  * `MOSTLY_Q3_K_L = 13`
  * `MOSTLY_Q4_K_S = 14`
  * `MOSTLY_Q4_K_M = 15`
  * `MOSTLY_Q5_K_S = 16`
  * `MOSTLY_Q5_K_M = 17`
  * `MOSTLY_Q6_K = 18`

#### Метаданные источника

Информация о том, откуда эта модель была взята. Это полезно для отслеживания происхождения модели и для поиска оригинального источника, если модель была изменена. Для модели, которая была преобразована из GGML, например, эти ключи будут указывать на модель, из которой она была преобразована.

* `general.source.url: string`: URL-адрес источника модели. Может быть репозиторием GitHub, статьей и т.д.
* `general.source.huggingface.repository: string`: Репозиторий моделей Hugging Face, на котором размещена или основана эта модель

### LLM

В следующем `[llm]` используется для заполнения имени конкретной архитектуры LLM. Например, `llama` для LLaMA, `mpt` для MPT и т.д. Если упоминается в разделе архитектуры, это обязательно для этой архитектуры, но не все ключи обязательны для всех архитектур. Обратитесь в соответствующий раздел для получения дополнительной информации.

* `[llm].context_length: uint64`: Также известен как `n_ctx`. длина контекста (в маркерах), на котором была обучена модель. Для большинства архитектур это жесткий лимит на длину ввода. Архитектуры, такие как RWKV, которые не зависят от внимания стиля трансформера, могут обрабатывать более длинные входы, но это не гарантируется.
* `[llm].embedding_length: uint64`: Также известен как `n_embd`. Размер слоя вложения.
* `[llm].block_count: uint64`: Количество блоков внимания+слоев с обратной связью (т.е. основная часть LLM). Не включает в себя входные или слои вложения.
* `[llm].feed_forward_length: uint64`: Также известен как `n_ff`. Длина слоя с обратной связью.
* `[llm].use_parallel_residual: bool`: Использовать ли логику остаточных параллелей.
* `[llm].tensor_data_layout: string`: При преобразовании модели в GGUF тензоры могут быть переупорядочены для повышения производительности. Этот ключ описывает расположение данных тензора. Это необязательно; если отсутствует, считается, что это `reference`.
  * `reference`: тензоры расположены в том же порядке, что и в исходной модели
  * дополнительные варианты можно найти для каждой архитектуры в соответствующих разделах
* `[llm].expert_count: uint32`: Количество экспертов в моделях MoE (необязательно для не-MoE архитектур).
* `[llm].expert_used_count: uint32`: Количество экспертов, используемых при оценке каждого маркера (необязательно для не-MoE архитектур).

#### Внимание

* `[llm].attention.head_count: uint64`: Также известен как `n_head`. Количество головок внимания.
* `[llm].attention.head_count_kv: uint64`: Количество голов на группу, используемых в Grouped-Query-Attention. Если отсутствует или равен `[llm].attention.head_count`, модель не использует GQA.
* `[llm].attention.max_alibi_bias: float32`: Максимальное смещение для ALiBI.
* `[llm].attention.clamp_kqv: float32`: Значение (`C`), при котором значения тензоров `Q`, `K` и `V` обрезаются между (`[-C, C]`).
* `[llm].attention.layer_norm_epsilon: float32`: Эпсилон нормализации слоя.
* `[llm].attention.layer_norm_rms_epsilon: float32`: Эпсилон нормализации RMS слоя.
* `[llm].attention.key_length: uint32`: Необязательный размер ключевой головы, $d\_k$. Если не указано, будет `n_embd / n_head`.
* `[llm].attention.value_length: uint32`: Необязательный размер головы значения, $d\_v$. Если не указано, будет `n_embd / n_head`.

#### RoPE

* `[llm].rope.dimension_count: uint64`: Количество измерений RoPE.
* `[llm].rope.freq_base: float32`: Базовая частота RoPE.

##### Масштабирование

Следующие ключи описывают параметры масштабирования RoPE:

* `[llm].rope.scaling.type: string`: Может быть `none`, `linear`, или `yarn`.
* `[llm].rope.scaling.factor: float32`: Масштабирующий коэффициент RoPE для корректировки длины контекста.
* `[llm].rope.scaling.original_context_length: uint32_t`: Оригинальная длина контекста базовой модели.
* `[llm].rope.scaling.finetuned: bool`: Истинно, если модель была донастроена с масштабированием RoPE.

Обратите внимание, что более старые модели могут не иметь этих ключей и вместо этого использовать следующий ключ:

* `[llm].rope.scale_linear: float32`: Линейный масштабирующий коэффициент RoPE для корректировки длины контекста.

Рекомендуется использовать новые ключи, если это возможно, так как они более гибкие и позволяют более сложные схемы масштабирования. Исполнители должны поддерживать оба варианта навсегда.

#### SSM

* `[llm].ssm.conv_kernel: uint32`: Размер сдви��а состояния.
* `[llm].ssm.inner_size: uint32`: Размер вложения состояний.
* `[llm].ssm.state_size: uint32`: Размер рекуррентного состояния.
* `[llm].ssm.time_step_rank: uint32`: Ранг шагов времени.

#### Модели

Следующие разделы описывают метаданные для каждой архитектуры модели. Каждый указанный ключ _должен_ присутствовать.

##### LLaMA

* `llama.context_length`
* `llama.embedding_length`
* `llama.block_count`
* `llama.feed_forward_length`
* `llama.rope.dimension_count`
* `llama.attention.head_count`
* `llama.attention.layer_norm_rms_epsilon`

###### Необязательно

* `llama.rope.scale`
* `llama.attention.head_count_kv`
* `llama.tensor_data_layout`:
  * `Meta AI original pth`:
    ```python
    def permute(weights: NDArray, n_head: int) -> NDArray:
        return (weights.reshape(n_head, 2, weights.shape[0] // n_head // 2, *weights.shape[1:])
                    .swapaxes(1, 2)
                    .reshape(weights.shape))
    ```
* `llama.expert_count`
* `llama.expert_used_count`

##### MPT

* `mpt.context_length`
* `mpt.embedding_length`
* `mpt.block_count`
* `mpt.attention.head_count`
* `mpt.attention.alibi_bias_max`
* `mpt.attention.clip_kqv`
* `mpt.attention.layer_norm_epsilon`

##### GPT-NeoX

* `gptneox.context_length`
* `gptneox.embedding_length`
* `gptneox.block_count`
* `gptneox.use_parallel_residual`
* `gptneox.rope.dimension_count`
* `gptneox.attention.head_count`
* `gptneox.attention.layer_norm_epsilon`

###### Необязательно

* `gptneox.rope.scale`

##### GPT-J

* `gptj.context_length`
* `gptj.embedding_length`
* `gptj.block_count`
* `gptj.rope.dimension_count`
* `gptj.attention.head_count`
* `gptj.attention.layer_norm_epsilon`

###### Необязательно

* `gptj.rope.scale`

##### GPT-2

* `gpt2.context_length`
* `gpt2.embedding_length`
* `gpt2.block_count`
* `gpt2.attention.head_count`
* `gpt2.attention.layer_norm_epsilon`

##### BLOOM

* `bloom.context_length`
* `bloom.embedding_length`
* `bloom.block_count`
* `bloom.feed_forward_length`
* `bloom.attention.head_count`
* `bloom.attention.layer_norm_epsilon`

##### Falcon

* `falcon.context_length`
* `falcon.embedding_length`
* `falcon.block_count`
* `falcon.attention.head_count`
* `falcon.attention.head_count_kv`
* `falcon.attention.use_norm`
* `falcon.attention.layer_norm_epsilon`

###### Необязательно

* `falcon.tensor_data_layout`:

  * `jploski` (автор оригинальной реализации GGML Falcon):

    ```python
    # The original query_key_value tensor contains n_head_kv "kv groups",
    # each consisting of n_head/n_head_kv query weights followed by one key
    # and one value weight (shared by all query heads in the kv group).
    # This layout makes it a big pain to work with in GGML.
    # So we rearrange them here,, so that we have n_head query weights
    # followed by n_head_kv key weights followed by n_head_kv value weights,
    # in contiguous fashion.

    if "query_key_value" in src:
        qkv = model[src].view(
            n_head_kv, n_head // n_head_kv + 2, head_dim, head_dim * n_head)

        q = qkv[:, :-2 ].reshape(n_head * head_dim, head_dim * n_head)
        k = qkv[:, [-2]].reshape(n_head_kv * head_dim, head_dim * n_head)
        v = qkv[:, [-1]].reshape(n_head_kv * head_dim, head_dim * n_head)

        model[src] = torch.cat((q,k,v)).reshape_as(model[src])
    ```

##### Mamba

* `mamba.context_length`
* `mamba.embedding_length`
* `mamba.block_count`
* `mamba.ssm.conv_kernel`
* `mamba.ssm.inner_size`
* `mamba.ssm.state_size`
* `mamba.ssm.time_step_rank`
* `mamba.attention.layer_norm_rms_epsilon`

##### RWKV

Размер словарного запаса совпадает с количеством строк в матрице `head`.

* `rwkv.architecture_version: uint32`: Единственное допустимое значение в настоящее время - 4. Версия 5 ожидается в будущем.
* `rwkv.context_length: uint64`: Длина контекста, используемая при обучении или донастройке. RWKV может обрабатывать более длинный контекст, чем этот предел, но качество вывода может страдать.
* `rwkv.block_count: uint64`
* `rwkv.embedding_length: uint64`
* `rwkv.feed_forward_length: uint64`

##### Whisper

Ключи, у которых не определены типы, должны считаться совпадающими с определениями `llm.` ключей. (Например, `whisper.context_length` эквивалентно `llm.context_length`.) Это потому, что они обе являются трансформерными моделями.

* `whisper.encoder.context_length`
* `whisper.encoder.embedding_length`
* `whisper.encoder.block_count`
* `whisper.encoder.mels_count: uint64`
* `whisper.encoder.attention.head_count`

* `whisper.decoder.context_length`
* `whisper.decoder.embedding_length`
* `whisper.decoder.block_count`
* `whisper.decoder.attention.head_count`

#### Запросы

**TODO**: Включить формат запроса и/или метаданные о том, как он должен использоваться (инструкция, разговор, автозаполнение и т.д.).

### LoRA

**TODO**: Определить, какие метаданные необходимы для LoRA. Вероятно, желательные возможности:

* точно соответствовать существующей модели, чтобы она не могла быть применена неверно
* быть помеченным как LoRA, чтобы исполнители не пытались запустить его самостоятельно

Должна ли это быть архитектура, или должна ли она делить детали оригинальной модели с дополнительными полями для обозначения ее как LoRA?

### Токенайзер

Следующие ключи используются для описания токенайзера модели. Рекомендуется авторам моделей поддерживать как можно больше из этих ключей, так как это позволит обеспечить лучшее качество токенизации с поддерживаемыми исполнителями.

#### GGML

GGML поддерживает встроенный словарный запас, который позволяет выводить модель, но реализации токенизации с использованием этого словарного запаса (например, `llama.cpp` токенайзер) могут иметь низкую точность по сравнению с оригинальным токенайзером, использованным для модели. Ко��да доступен более точный токенайзер и поддерживается, он должен использоваться вместо этого.

Не гарантируется, что он будет стандартизирован для всех моделей, и он может измениться в будущем. Рекомендуется авторам моделей использовать более стандартизированный токенайзер, если это возможно.

* `tokenizer.ggml.model: string`: Имя модели токенайзера.
  * `llama`: Llama style SentencePiece (tokens and scores extracted from HF `tokenizer.model`)
  * `replit`: Replit style SentencePiece (tokens and scores extracted from HF `spiece.model`)
  * `gpt2`: GPT-2 / GPT-NeoX style BPE (tokens extracted from HF `tokenizer.json`)
  * `rwkv`: RWKV tokenizer
* `tokenizer.ggml.tokens: array[string]`: Список токенов, индексированных по идентификатору токена, используемому моделью.
* `tokenizer.ggml.scores: array[float32]`: Если присутствует, вероятность каждого токена. Если отсутствует, все токены считаются равновероятными. Если присутствует, он должен иметь ту же длину и индекс, что и `tokens`.
* `tokenizer.ggml.token_type: array[int32]`: Тип токена (1=нормальный, 2=неизвестный, 3=управляющий, 4=определенный пользователем, 5=неиспользуемый, 6=байт). Если присутствует, он должен иметь ту же длину и индекс, что и `tokens`.
* `tokenizer.ggml.merges: array[string]`: Если присутствует, слияния токенайзера. Если отсутствует, токены считаются атомарными.
* `tokenizer.ggml.added_tokens: array[string]`: Если присутствует, токены, добавленные после обучения.

##### Специальные токены

* `tokenizer.ggml.bos_token_id: uint32`: Маркер начала последовательности
* `tokenizer.ggml.eos_token_id: uint32`: Маркер конца последовательности
* `tokenizer.ggml.unknown_token_id: uint32`: Неизвестный токен
* `tokenizer.ggml.separator_token_id: uint32`: Разделительный токен
* `tokenizer.ggml.padding_token_id: uint32`: Токен заполнения

#### Hugging Face

Hugging Face поддерживает собственную библиотеку `tokenizers`, которая поддерживает широкий спектр токенайзеров. Если ваш исполнитель использует эту библиотеку, он может использовать токенайзер модели напрямую.

* `tokenizer.huggingface.json: string`: все содержимое HF `tokenizer.json` для конкретной модели (например, <https://huggingface.co/mosaicml/mpt-7b-instruct/blob/main/tokenizer.json>). Включено для совместимости с исполнителями, поддерживающими токенайзеры HF напрямую.

#### Другие

Могут использоваться другие токенайзеры, но они не обязательно стандартизированы. Они могут быть специфичными для исполнителя. Они будут документироваться здесь по мере их открытия/дальнейшего развития.

* `tokenizer.rwkv.world: string`: RWKV World токенайзер, как этот <https://github.com/BlinkDL/ChatRWKV/blob/main/tokenizer/rwkv_vocab_v20230424.txt>. Этот текстовый файл должен включаться дословно.
* `tokenizer.chat_template : string`: шаблон Jinja, который определяет формат ввода, ожидаемый моделью. Для получения дополнительной информации см.: <https://huggingface.co/docs/transformers/main/en/chat_templating>

### Граф вычислений

Это будущее расширение, которое все еще нуждается в обсуждении, и может потребовать новой версии GGUF. На момент написания основным препятствием является стабилизация формата графа вычислений.

В граф вычислений GGML узлов может быть включен в саму модель, что позволит исполнителю запускать модель без предоставления собственной реализации архитектуры. Это позволит обеспечить более последовательный опыт работы на разных исполнителях и позволит поддерживать более сложные архитектуры без необходимости для исполнителя реализовывать их.

## Стандартизированные имена тензоров

Для минимизации сложности и максимизации совместимости рекомендуется, чтобы модели, использующие архитектуру трансформера, использовали следующую конвенцию именования для своих тензоров:

### Базовые слои

`AA.weight` `AA.bias`

где `AA` может быть:

* `token_embd`: Слой вложения токенов
* `pos_embd`: Слой вложения позиций
* `output_norm`: Слой нормализации вывода
* `output`: Выходной слой

### Блоки слоев внимания и питания вперед

`blk.N.BB.weight` `blk.N.BB.bias`

где N обозначает номер блока, которому принадлежит слой, а `BB` может быть:

* `attn_norm`: Слой нормализации внимания
* `attn_norm_2`: Слой нормализации внимания
* `attn_qkv`: Слой запроса-ключ-значение внимания
* `attn_q`: Слой запроса внимания
* `attn_k`: Слой ключа внимания
* `attn_v`: Слой значения внимания
* `attn_output`: Слой вывода внимания

* `ffn_norm`: Слой нормализации питания вперед
* `ffn_up`: Слой "вверх" питания вперед
* `ffn_gate`: Слой "шлюз" питания вперед
* `ffn_down`: Слой "вниз" питания вперед
* `ffn_gate_inp`: Слой маршрутизации экспертов для слоя питания вперед в моделях MoE
* `ffn_gate_exp`: Слой "шлюз" слоя питания вперед на эксперта в моделях MoE
* `ffn_down_exp`: Слой "вниз" слоя питания вперед на эксперта в моделях MoE
* `ffn_up_exp`: Слой "вверх" слоя питания вперед на эксперта в моделях MoE

* `ssm_in`: Слой входных проекций модели состояний пространства
* `ssm_conv1d`: Слой сдвига/свертки модели состояний пространства
* `ssm_x`: Слой селективной параметризации модели состояний пространства
* `ssm_a`: Слой сжатия состояний модели состояний пространства
* `ssm_d`: Слой пропуска соединения модели состояний пространства
* `ssm_dt`: Слой шага времени модели состояний пространства
* `ssm_out`: Слой выходных проекций модели состояний пространства

## История версий

Этот документ активно обновляется для описания текущего состояния метаданных, и эти изменения не отслеживаются вне коммитов.

Однако сам формат _изменился_. Следующие разделы описывают изменения в самом формате.

### v3

Добавлена поддержка big-endian.

### v2

Большинство счетчиков (длин и т.д.) были изменены с `uint32` на `uint64`, чтобы поддерживать более крупные модели в будущем.

### v1

Исходная версия.

## Историческое состояние дел

Предоставляется следующая информация для контекста, но она не является необходимой для понимания остальной части этого документа.

### Обзор

В настоящее время существует три формата файлов GGML для LLM:

* **GGML** (неверсионный): базовый формат, без версионирования или выравнивания.
* **GGMF** (версионный): то же, что и GGML, но с версионированием. Существует только одна версия.
* **GGJT**: Выравнивает тензоры для использования с `mmap`, которая требует выравнивания. v1, v2 и v3 идентичны, но более поздние версии используют другую схему квантизации, несовместимую с предыдущими версиями.

GGML в основном используется примерами в `ggml`, в то время как GGJT используется моделями `llama.cpp`. Другие исполнители могут использовать любой из трех форматов, но это не поддерживается "официально".

Эти форматы имеют одинаковую основную структуру:

* магическое число с необязательным номером версии
* гиперпараметры, специфичные для модели, включая
  * метаданные о модели, такие как количество слоев, количество голов и т.д.
  * `ftype`, описывающий тип большинства тензоров
    * для файлов GGML версия квантизации кодируется в `ftype`, деленном на 1000
* встроенный словарный запас, который является ��писком строк с предваряющей длиной. Форматы GGMF/GGJT встраивают плавающую точку с одинарной точностью рядом со строками.
* наконец, список тензоров с их предваряющим именем длины, типом и (выровненными, в случае GGJT) данными тензора

Отметьте, что эта структура не идентифицирует, какой архитектуре модели принадлежит модель, и не предоставляет никакой гибкости для изменения структуры гиперпараметров. Это означает, что единственный способ добавить новые гиперпараметры - это добавить их в конец списка, что является несовместимым изменением для существующих моделей.

### Недостатки

К сожалению, за последние несколько месяцев стали очевидными несколько проблем с существующими моделями:

* Невозможно определить, какой архитектуре модели принадлежит конкретная модель, потому что эта информация отсутствует
  * Соответственно, существующие программы не могут интеллектуально сбоить при встрече с новыми архитектурами
* Добавление или удаление новых гиперпараметров является несовместимым изменением, которое невозможно обнаружить читателю без использования эвристик
* Каждой архитектуре модели требуется свой скрипт преобразования в свой вариант GGML
* Поддержка обратной совместимости без нарушения структуры формата требует хитрых трюков, таких как упаковка версии квантизации в `ftype`, которые не гарантируется, что будут подхвачены читателями/писателями, и не являются последовательными между двумя форматами

### А почему не другие форматы?

Существуют другие форматы, которые можно было бы использовать, но проблемы включают:

* необходимость дополнительных зависимостей для загрузки или сохранения модели, что сложно в среде C
* ограниченная или отсутствующая поддержка 4-битной квантизации
* существующие культурные ожидания (например, является ли модель каталогом или файлом)
* отсутствие поддержки встроенных словарных запасов
* отсутствие контроля над направлением будущего развития

В конечном итоге вероятно, что GGUF будет необходим в обозримом будущем, и лучше иметь один формат, который хорошо документирован и поддерживается всеми исполнителями, чем искажать существующий формат для соответствия требованиям GGML.
